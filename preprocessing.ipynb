{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45f4185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split    \n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import string, emoji\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85c0c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\VINOTH KUMAR\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\VINOTH KUMAR\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\VINOTH KUMAR\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\VINOTH KUMAR M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "012e4415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet label\n",
       "0  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1  States reported 1121 deaths a small rise from ...  real\n",
       "2  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
       "3  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
       "4  Populous states can generate large case counts...  real"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = ['tweet', 'label']\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.to_csv(\"./data.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376f7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pos_tag_to_wordnet(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    if tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    if tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "610575d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordsegment import load, segment\n",
    "import re\n",
    "\n",
    "load()\n",
    "\n",
    "def replace_hashtag(text):\n",
    "    def _split_hash(match):\n",
    "        h = match.group(0)[1:]  # remove '#'\n",
    "        h_clean = re.sub(r'[^a-zA-Z]', '', h)  # keep only letters\n",
    "        if not h_clean:\n",
    "            return \"\"\n",
    "        try:\n",
    "            pieces = segment(h_clean)\n",
    "            return \" \".join(pieces) if pieces else h_clean\n",
    "        except ValueError:\n",
    "            # If segmentation fails, just return original cleaned hashtag\n",
    "            return h_clean\n",
    "    return re.sub(r'#\\w+', _split_hash, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "777a7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text_improved(text, remove_stopwords=True, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Returns a cleaned string.\n",
    "    - replace URLs, mentions\n",
    "    - demojize emojis (emoji -> :smile:)\n",
    "    - expand hashtags into words\n",
    "    - normalize repeated characters\n",
    "    - replace numbers with <NUM>\n",
    "    - remove punctuation (keeps internal apostrophes removed)\n",
    "    - lower, tokenize, optional stopword removal and lemmatize\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # 1. lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. urls and mentions\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)          # remove URLs\n",
    "    text = re.sub(r'@\\w+', ' ', text)                     # remove @mentions\n",
    "\n",
    "    # 3. demojize (turn emoji into textual token like :smile:)\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))    # \"ðŸ™‚\" -> \" :slightly_smiling_face: \"\n",
    "    \n",
    "    # 4. hashtags -> split into words\n",
    "    text = replace_hashtag(text)\n",
    "\n",
    "    # 5. replace digits with <NUM>\n",
    "    text = re.sub(r'\\d+(?:[\\.,]\\d+)*', ' <NUM> ', text)\n",
    "\n",
    "    # 6. normalize elongated characters: reduce 3+ repeats to 2 (so looove -> loove)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "    # 7. remove punctuation (keep spaces). If you want to keep emoticon punctuation, adjust.\n",
    "    # Remove punctuation except the placeholder tokens like :smile: and <NUM>\n",
    "    # First preserve coloned tokens (e.g. :smile:) by temporary marker\n",
    "    text = re.sub(r':([a-z0-9_+-]+):', r' EMOJI_\\1_EMO ', text)\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # restore emoji tokens\n",
    "    text = re.sub(r'EMOJI_([a-z0-9_+-]+)_EMO', r':\\1:', text)\n",
    "\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    # 8. tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 9. optional stopword removal, but keep <NUM> and emoji tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if remove_stopwords:\n",
    "        filtered = []\n",
    "        for t in tokens:\n",
    "            if t == '<NUM>' or (t.startswith(':') and t.endswith(':')):\n",
    "                filtered.append(t)\n",
    "            elif t not in stop_words:\n",
    "                filtered.append(t)\n",
    "        tokens = filtered\n",
    "\n",
    "    # 10. lemmatize with POS\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    if lemmatize:\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        lem_tokens = []\n",
    "        for tok, tag in pos_tags:\n",
    "            wn_tag = _pos_tag_to_wordnet(tag)\n",
    "            tok_lem = lemmatizer.lemmatize(tok, wn_tag)\n",
    "            lem_tokens.append(tok_lem)\n",
    "        tokens = lem_tokens\n",
    "\n",
    "    # 11. final join\n",
    "    cleaned = \" \".join(tokens)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "225a180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, text_column='tweet', label_column='label'):\n",
    "    \"\"\"\n",
    "    Adds 'clean_text' column (or overwrite) and drops rows with empty cleaned text.\n",
    "    Returns cleaned dataframe and (optionally) label encoder if needed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[text_column] = df[text_column].apply(preprocess_text_improved)\n",
    "    # drop rows where cleaned text is empty\n",
    "    before = len(df)\n",
    "    df = df[df[text_column].str.strip().astype(bool)].reset_index(drop=True)\n",
    "    after = len(df)\n",
    "    print(f\"Preprocessing: dropped {before-after} empty rows (out of {before}).\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec8d2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweet']\n",
    "y = df['label']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "tfidf_vectorizer =  TfidfVectorizer(\n",
    "    ngram_range=(1, 2),       # unigrams, bigrams, trigrams\n",
    "    min_df=5,                 # ignore words in <3 documents\n",
    "    max_df=0.7,               # ignore words in >80% of documents\n",
    "    sublinear_tf=True,        # logarithmic term frequency scaling\n",
    "    max_features=2000,       # limit features to top 20k by term frequency\n",
    "    norm='l2'                 # L2 normalization (good for linear models)\n",
    ")\n",
    "train_tfidf = tfidf_vectorizer.fit(df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fef87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit(Y_train)\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "336d761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: dropped 0 empty rows (out of 1060).\n",
      "Preprocessing: dropped 0 empty rows (out of 9540).\n",
      "label\n",
      "real    5002\n",
      "fake    4538\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dir = './split'\n",
    "os.makedirs(dir, exist_ok=True)\n",
    "train_df = pd.concat([X_train, Y_train], axis=1)\n",
    "test_df = pd.concat([X_test, Y_test], axis=1)\n",
    "test_df = preprocess_dataframe(test_df)\n",
    "test_df.to_csv('./split/test.csv', index=False)\n",
    "train_df = preprocess_dataframe(train_df)\n",
    "train_df.to_csv('./split/train.csv')\n",
    "train_df_encoded = tfidf_vectorizer.transform(train_df['tweet'])\n",
    "train_df_encoded = pd.DataFrame(train_df_encoded.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "train_df_encoded.to_csv('./split/train_df_encoded.csv', index=False)\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1582c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VINOTH KUMAR M\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "size = train_df.shape[0] // 5\n",
    "dir = './tfidf'\n",
    "dir1 = './preprocess'\n",
    "os.makedirs(dir, exist_ok=True)\n",
    "os.makedirs(dir1, exist_ok=True)\n",
    "# Shuffle the dataset once\n",
    "shuffled_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into 4 equal parts (last split may have 1 extra row if not divisible)\n",
    "splits = np.array_split(shuffled_df, 4)\n",
    "\n",
    "for i, split_df in enumerate(splits, start=1):\n",
    "    # Save raw text\n",
    "    split_df.to_csv(f'./split/train_{i}.csv', index=False)\n",
    "    \n",
    "    # Transform to TF-IDF\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(split_df['tweet'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                            columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    tfidf_df.to_csv(f'./tfidf/train_{i}.csv', index=False)\n",
    "\n",
    "test_df = pd.read_csv('./split/test.csv')\n",
    "test_df = tfidf_vectorizer.transform(test_df['tweet'])\n",
    "test_df = pd.DataFrame(test_df.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "test_df.to_csv('./tfidf/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
